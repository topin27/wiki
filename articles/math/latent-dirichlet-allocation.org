* 简述

#+BEGIN_QUOTE
  Documents are assumed to be composed of mixtures of topics, which are
  in turn composed of mixtures of words. If we knew the topic and
  document distributions, we could generate new documents using a
  probabilistic model. In LDA, we run this process in reverse to infer
  the topic and document distributions given the documents.
#+END_QUOTE

* 基础知识

** 伯努利分布

即是简单的“两点分布”，以概率(p)和(1-p)的概率取0和1值。其(E\_{x}=p)，(D\_{x}=p(1-p))。

** 二项分布

重复n次的伯努利实验，通常记为(X{\sim}b(n,p))，其概率密度函数为：[P(K=k)={\lgroup}/{k}^{{n}{\rgroup}p}{k}(1-p)\^{n-k}]其中(\lgroup/{k}\^{n}\rgroup=C(n,k)=\frac{n!}{k!(n-k)!})是二项式系数。

** 多项分布

二项分布扩展到多维的情况。单次试验的值不再是0-1，而是由多个可能的值，即(\sum\_{i=1}^{{k}{p\_i}=1,p\_i>0)，其概率密度函数为：[p(x\_1,...,x\_k;n,p\_1,...,p\_k)={\frac{n!}{x_{1}!\cdots{x_{k}!}}}{p\_{1}}{x\_1}\cdots{p_{k}^{x_k}}}]

** (\beta)分布

(\beta)分布是一个关于概率的概率分布，观察一系列的二项分布，但是每个二项分布的(n,p)都是未知的情况下，成功率p的分布。其中(\alpha)为成功事件数，(\beta)为失败事件数。取值范围为([0,1])的随机变量x的概率密度函数为[f(x;\alpha,\beta)=\frac{1}{B(\alpha,\beta)}x^{{\alpha-1}(1-x)}{\beta-1}]其中：[\frac{1}{B(\alpha,\beta)}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\Gamma(z)=\int\_0^{{\infty}{t}{z-1}e\^{-t}dt}]，这里的(\Gamma(x))即为gamma函数。

(\beta)分布的均值是(E\_x=\frac{\alpha}{\alpha+\beta})，方差是(D\_x=\frac{\alpha{\beta}}{(\alpha+\beta)^2(\alpha+\beta+1)})

** Dirichlet分布

即是(\beta)分布在高维度上的推广，其概率密度函数为：[f(x\_1,x\_2,...,x\_k;\alpha\_1,\alpha\_2,...,\alpha/k)={\frac{1}{B(\alpha)}}{\prod/{i=1}^{{k}{x\_i}{\alpha\^i-1}}}]其中[B(\alpha)=\frac{\prod_{i=1}^{k}{\Gamma(\alpha^i)}}{\Gamma{\sum_{i=1}^{k}{\alpha}^i}},\sum{x_i}=1]

** 阶乘在实数上的推广

[n!=\Gamma(n+1)=\int\_0^{{\infty}{e}{-t}t\^{n}dt}]其在实数上的推广[\Gamma(x)=\int\_0^{{\infty}{t}{x-1}e\^{-t}dt}]即为欧拉Gamma函数。

** 贝叶斯学派的思考方式

[先验分布\pi(\theta)+样本信息\chi\Rightarrow后验分布\pi(\theta|x)]

在新样本到来之前，人们对(\theta)的认知是先验分布(\pi(\theta))，在得到新样本(\chi)后，人们对(\theta)的认知为(\pi(\theta|x))。

** 共轭先验分布

在贝叶斯理论体系中，如果后验概率(p(\theta|x))和先验概率(p(\theta))满足同样的分布律，那么先验和后验分布叫做满足共轭分布，同时先验分布叫做似然函数的共轭先验分布。

** EM算法

* 参考

-  [[https://blog.csdn.net/v_july_v/article/details/41209515][通俗理解LDA主题模型]]
