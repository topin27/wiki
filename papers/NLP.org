--------------

* Event Extraction

** 文献

*** TODO Event Extraction - TAMU Computer Science People Pages

[[http://faculty.cse.tamu.edu/huangrh/Fall17/l19_event-extraction.pdf][链接地址]]，和关心的事件比较相关。

*** TODO Joint Extraction of Events and Entities within a Document

Context. (Yang B, Mitchell T M)

代码地址：[[https://github.com/bishanyang/EventEntityExtractor][bishanyang/EventEntityExtractor]]

** 数据集

* Open Information Extraction

** 文献

*** TODO Leveraging Linguistic Structure For Open Domain Information

Extraction. (Gabor Angeli, Melvin Johnson Premkumar, and Christopher D.
Manning)

corenlp中 [[https://nlp.stanford.edu/software/openie.html][OpenIE]]
模块根据该论文实现，论文地址：[[https://nlp.stanford.edu/pubs/2015angeli-openie.pdf][link]]

* 主题提取

** 文献

*** TODO 基于LDA模型的博客主题提取. (王珍)

北大的一篇关于主题提取的硕士毕业论文。大致内容如标题所述。

*** TODO Exploratory analysis of highly heterogeneous document

collections. (Maiya A S, Thompson J P, Loaizalemos F, et al)

* 评论倾向与情感分析

** 文献

*** 基于层次结构的多策略中文微博情感分析和特征抽取.

(谢丽星，周明，孙茂松)

1. 相关工作

   -  主题无关的情感分析

      -  基于词典的方法。需要构建情感词典，然后统计文本中情感词的正负差值进行情感判定。
         无法解决未登录词的问题。
      -  有监督的学习方法。
      -  无监督的学习方法。选定基本的情感词，然后制定模板来提取短语，计算这些短语与
         基本情感词之间的关联度，根据正负向关联度的差值来确定情感极性。

   -  主题相关的情感分析

      -  基于规则的方法。对形容词、动词、名词制定一系列规则来判定文本的极性。
      -  基于特征的方法。不仅要确定情感属性，还需要确定每个情感属性涉及的产品。

2. 算法设计

   -  基于表情符号的方法。统计正负情感表情符号的个数，根据个数差值来确定极性。

   -  基于情感词典的规则方法。统计正负情感词的个数，根据个数差值来确定极性。

   -  基于层次结构的多策略分析方法。

      -  对文本不分句，将一条微博作为一个整体：

         -  一步三分类：使用SVM直接进行正中负三分类；
         -  二步分类：先建立模型对微博的主、客观情况进行分类，然后在此基础上提取
            极性特征，进一步在分为正负情感倾向。

      -  分句，将一条微博拆分成若干个句子，对每个句子进行分类：

         -  句子组成规则分类：对每句进行情感分类，然后统计所有句子的情感句差值。
         -  句子组成SVM分类：对每句进行情感分类，然后训练SVM分类器进行最终情感分类。

** 数据集

*** Movie Review Data

下载地址：https://www.cs.cornell.edu/people/pabo/movie-review-data/ ，
包含1000条正面评论和1000条负面评论，广泛应用于文本分类尤其是恶意评论识别方面。

* 垃圾邮件识别

** 数据集

*** SpamBase

下载地址：https://archive.ics.uci.edu/ml/datasets/Spambase ，
不是原始的邮件数据，而是已经特征化的数据，对应的特征是统计的关键字以及特殊符号。

*** Enron

下载地址：https://www.cs.cmu.edu/~./enron/ ，
真实环境下的真实邮件，由人工标注。用于垃圾邮件识别。Kaggle上也有这个数据集的
[[https://www.kaggle.com/wcukierski/enron-email-dataset][下载]]。

* 文本相似度

** 文献

*** DONE 基于语义理解的文本相似度算法. (金博, 史彦军, 滕弘飞)

在词语层次中，相似度用于衡量文本中词语的可替换程度，这里的词语相似度不等同于词
语的相关度，例如“军人”和“武器”两个词，其相似度非常低，但相关度却非常高。可以这
样认为，词语相似度反映的是词语之间的聚合特点，而词语相关度反映的是词语之间的组
合特点。

论文中的文本相似度的计算方法为，首先通过语义分析计算词语相似度，接着通过分词及
对句子结构进行分析计算句子相似，最后按照句子与段落之间的关系得到段落相似度的计
算方法。

1. 词语相似度

   使用知网中的义项概念来进行：假设两个词语$w<sub>1</sub>$和$w<sub>2</sub>$，如果$w<sub>1</sub>$上有n个义项
   $s<sub>11</sub>,s<sub>12</sub>,…,s<sub>1n</sub>$，$w<sub>2</sub>$上有m个义项$s<sub>21</sub>,s<sub>22</sub>,…,s{2m}$，则
   $w<sub>1</sub>$与$w<sub>2</sub>$之间的相似度定义为各个义项的相似度的最大值：
   [sim W(w\_1,w\_2)=max\_{i=1,...,n,j=1,...,m}sim WS(s\_{1i},s\_{2j})]
   其中$sim WS(s<sub>1</sub>,s<sub>2</sub>)$表示的是两个义项的相似度，而义项都是由义原表示，因此义项
   相似度转换为义原相似度的计算，根据某篇文献的公式，该轮为将义原相似度定义为：
   [sim WP(p\_1,p\_2)=\frac{\alpha}{d+\alpha}]
   其中d是$p<sub>1</sub>$和$p<sub>2</sub>$在义原层次体系中的路径长度，$*α*$是一个可调节的参数，
   其含义是相似度为0.5时的路径长度，论文中取(\alpha=1.6)

   考虑到汉语中实词才是表达文章意义的关键词汇，在相似度计算时忽略了虚词部分的相
   似度计算。然后将词的义原分为第一独立义原、其他独立义原、关系义原、符号义原分别
   计算相似度。最终两个义项语义表达式的整体相似度为： [sim
   WS(s\_1,s\_2)=\sum\_1\^4\beta\_i sim WP\_i(p\_1,p\_2)]

2. 句子相似度

   将句子中的词语根据词性进行分类（名词、动词、形容词、数词、量词），然后按照分类
   对两句话中的词语进行词语相似度计算，取出最大的相似度作为句子的相似度。

3. 段落相似度

   同句子相似度集成词语相似度的方式一致，段落相似度也以同样的方式集成句子相似度。

*** TODO 基于属性论的文本相似度计算. (潘谦红, 王炬, 史忠植)

* 文本匹配

** 文献

*** DONE 深度文本匹配综述. (庞亮, 兰艳艳, 徐君)

CLOSED: [2018-09-20 周四 16:43]

其中一个作者的一个简述PPT：[[http://www.bigdatalab.ac.cn/~junxu/publications/CCIR2016-tutorial.pdf][link]]

1. 引言

   深度文本匹配模型划分为3类：

   -  基于单语义文档表达的深度学习模型
   -  基于多语义文档表达的深度学习模型
   -  直接建模匹配模式的深度学习模型

2. 文本匹配问题简介

   衡量一个排序结果优劣的评价指标有：P@k(Precision at k), R@k(Recall at
   k), MAP(Mean Average Precision), MRR(Mean Reciprocal
   Rank)以及nDCG(normalized Discounted Cumulative Gain).

   定义真实排序前k个文本中，匹配文本的数量是$G<sub>k</sub>$，而在预测排序中前k个文本中，
   匹配文本的数量是$Y<sub>k</sub>$，评价指标P@k和R@k的定义如下：
   [P@k=\frac{Y_k}{k},R@k=\frac{Y_k}{G_k}]
   假设预测排序中的真实匹配的文本的排序位置为$k<sub>1</sub>,k<sub>2</sub>,…,k<sub>r</sub>$，其中r是整个列表
   中所有匹配文本的数量，那么指标MAP的定义如下：
   [MAP=\frac{\sum_{i=1}^r{P@k_i}}{r}]
   如果只考虑排名最靠前的真实匹配的文本$k<sub>1</sub>$，就可以到处指标MRR的定义：
   [MRR=P@k\_1]

3. 基于单语义文档表达的深度学习模型

   广义的说，传统方法得到的只基于一个文档的特征就可以看作是一个文档的表达，比如文
   档中的词频，文档的长度等。而基于单语义深度学习模型中的文档表达这是利用深度学习
   方法生成一个文档的高维稠密向量，得到向量之后，直接计算两个向量的相似度便可输出
   两个文档的匹配度。

4. 基于多语义文档表达的深度学习模型

   综合考虑文本的局部性表达（词、短语等）和全局性表达（句子）。这类模型不仅会考虑
   两端文本最终的表达向量的相似度，也会生成局部的短语或者更长的短语的表达进行匹配。
   这样多粒度的匹配可以很好的补充基于单语义文档表达的深度学习模型在压缩整个句子过
   程中的信息损失。

   1. 多粒度卷积神经网络

      使用卷积网络来分别得到词、短语和句子等几个不同层面的文本表达，然后将这些向量
      拼接到一起或者建模这些向量的相似度来得到最终的匹配值。

      首先将一个句子拆解成4个层次，单次级别、短语级别、长短语级别和句子级别，之后将
      两个句子不同级别的特征进行两两的相似度计算，得到相似度矩阵，进行动态最大值池化
      得到两个句子的相似度得分。

*** TODO A deep architecture for semantic matching with multiple

positional sentence representations. (Wan S, Lan Y, Guo J, et al)

1. Introduction

   A lot of deep models follow the paradigm to first represent the whole
   sentence to a single distributed representation, and then compute
   similarities between the two vectors to output the mathing score.
   Examples include DSSM, CDSSM, ARC-I, CNTN and LSTM-RNN. The main
   disadvantage lies in that important local information is lost when
   compressing such a complicated sentence into a single vector.

   som other owrks focus on taking multiple granularity, e.g. word,
   phrase, and sentence level representations, into consideration for
   the matching process. Examples include ARC-II, RAE, Deep-Match,
   Bi-CNN-MI and MultiGranCNN, but are still far from completely solving
   the matching problem. Because they are limited to well capture the
   contextualized local information, by directly involving word and
   phrase level representations.

2. Our Approach

   Firstly, each positional sentence representation is a sentence
   representation at one position, generated by a bidirectional long
   short term memory(Bi-LSTM); Secondly, the interactions between
   different positional sentence representations form a similarity
   matrix/tensor by different similarity functions; Lastly, the final
   matching score is produced by aggregating such interactions through
   k-Max pooling and a multilayer perceptron.
